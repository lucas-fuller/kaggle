{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30407,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lucasfuller96/hyperparameter-tuning-with-optuna?scriptVersionId=160300564\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-22T19:17:43.597453Z","iopub.execute_input":"2024-01-22T19:17:43.598718Z","iopub.status.idle":"2024-01-22T19:17:43.641349Z","shell.execute_reply.started":"2024-01-22T19:17:43.598648Z","shell.execute_reply":"2024-01-22T19:17:43.639774Z"}}},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport seaborn as sns\nsns.set_theme()\n\nimport optuna\nfrom optuna.samplers import TPESampler","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:43.643571Z","iopub.execute_input":"2024-01-22T19:17:43.64401Z","iopub.status.idle":"2024-01-22T19:17:45.655133Z","shell.execute_reply.started":"2024-01-22T19:17:43.643963Z","shell.execute_reply":"2024-01-22T19:17:45.653805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:45.657048Z","iopub.execute_input":"2024-01-22T19:17:45.657494Z","iopub.status.idle":"2024-01-22T19:17:45.756936Z","shell.execute_reply.started":"2024-01-22T19:17:45.657448Z","shell.execute_reply":"2024-01-22T19:17:45.755316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets take a look at the dataset!","metadata":{}},{"cell_type":"markdown","source":"From the Kaggle docs:\n- PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n- HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n- CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n- Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n- Destination - The planet the passenger will be debarking to.\n- Age - The age of the passenger.\n- VIP - Whether the passenger has paid for special VIP service during the voyage.\n- RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n- Name - The first and last names of the passenger.\n- Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:45.760064Z","iopub.execute_input":"2024-01-22T19:17:45.760633Z","iopub.status.idle":"2024-01-22T19:17:45.809034Z","shell.execute_reply.started":"2024-01-22T19:17:45.760589Z","shell.execute_reply":"2024-01-22T19:17:45.807478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:45.81049Z","iopub.execute_input":"2024-01-22T19:17:45.810854Z","iopub.status.idle":"2024-01-22T19:17:45.851846Z","shell.execute_reply.started":"2024-01-22T19:17:45.810819Z","shell.execute_reply":"2024-01-22T19:17:45.850308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a combined dataset so that we can make any preprocessing to both dataset at once.","metadata":{}},{"cell_type":"code","source":"train_len = train.shape[0]\ntest_len = test.shape[0]\n\ntest['Transported'] = False\n\ntrain_test = pd.concat([train,test], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:45.853543Z","iopub.execute_input":"2024-01-22T19:17:45.853974Z","iopub.status.idle":"2024-01-22T19:17:45.869863Z","shell.execute_reply.started":"2024-01-22T19:17:45.853932Z","shell.execute_reply":"2024-01-22T19:17:45.868219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having a quick look at the histograms of the spending features, we've got a large skew caused by a disproportionate number of 0 values, and large valued outliers. Since we're using Random Forest, we don't need to transform these numerical features, since Random Forest handles outliers and skewed distributions well, although it can help with visualisations so we will in this case. If we were using a non-tree based model (e.g. Logistic Regression), we would want to log transform the skewed features and add a maximum value to cap large outliers because this can aid in model learning.","metadata":{}},{"cell_type":"code","source":"train.hist(bins = 30, figsize=(20,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:45.871747Z","iopub.execute_input":"2024-01-22T19:17:45.872284Z","iopub.status.idle":"2024-01-22T19:17:47.810797Z","shell.execute_reply.started":"2024-01-22T19:17:45.872228Z","shell.execute_reply":"2024-01-22T19:17:47.809104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we've created a function to help us visualise the correlation between a feature and the target variable.","metadata":{}},{"cell_type":"code","source":"def feature_to_target(dataset, columns, figsize=(20,10), bins=50, remove_0_values=False):\n    \n    plt.figure(figsize=figsize)\n    \n    for n, column in enumerate(columns):\n        \n        length = int(np.round(len(columns) ** 0.5))\n        \n        plt.subplot(length, length+1,n+1)\n    \n        if remove_0_values == True:\n            \n            mask = dataset[column] > 0\n            sns.histplot(dataset[mask],x=column, hue=\"Transported\", multiple=\"stack\", bins=bins)\n        \n        else:\n            \n            sns.histplot(dataset,x=column, hue=\"Transported\", multiple=\"stack\", bins=bins)\n        \n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:47.816677Z","iopub.execute_input":"2024-01-22T19:17:47.817604Z","iopub.status.idle":"2024-01-22T19:17:47.829848Z","shell.execute_reply.started":"2024-01-22T19:17:47.817543Z","shell.execute_reply":"2024-01-22T19:17:47.828404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age","metadata":{}},{"cell_type":"markdown","source":"The Age distribution looks to have distinct age bandings, so we will be splitting this into a categorical feature for discrete age bandings.","metadata":{}},{"cell_type":"code","source":"age_bins = int(train_test['Age'].max() - train_test['Age'].min())\n\nfeature_to_target(train_test[0:train_len], ['Age'], figsize=(20,10), bins=age_bins)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:47.831799Z","iopub.execute_input":"2024-01-22T19:17:47.833064Z","iopub.status.idle":"2024-01-22T19:17:48.704457Z","shell.execute_reply.started":"2024-01-22T19:17:47.833002Z","shell.execute_reply":"2024-01-22T19:17:48.702983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill na's with mean age from training set\ntrain_test[\"Age\"] = train_test[\"Age\"].fillna(train_test.iloc[0:train_len][\"Age\"].mean())\n\n#Then create new variable that groups Age into groups\nbins = [-1, 4, 12, 17, 24, 29, 39, 49, 59, 69, 79]\nnames = ['0-4', '5-12', '13-17', '18-24', '25-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n\ntrain_test['AgeBand'] = pd.cut(train_test['Age'], bins, labels=names)\n\ntrain_test.drop('Age', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:48.711459Z","iopub.execute_input":"2024-01-22T19:17:48.711982Z","iopub.status.idle":"2024-01-22T19:17:48.733569Z","shell.execute_reply.started":"2024-01-22T19:17:48.711935Z","shell.execute_reply":"2024-01-22T19:17:48.732004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passengers younger than 18 have a higher chance of being transported, with the 0-4 range having the higher proportion of transported passengers. Those in 18-24 and 30-39 are more likely to not be transported, with the rest having fairly even splits.","metadata":{}},{"cell_type":"code","source":"sns.catplot(data=train_test[0:train_len], x=\"AgeBand\", kind=\"count\", hue='Transported')\n\nplt.xticks(rotation=90)\nplt.title('Distribution of Transported Passengers for the new Age Band Feauture')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:48.736191Z","iopub.execute_input":"2024-01-22T19:17:48.737001Z","iopub.status.idle":"2024-01-22T19:17:49.430118Z","shell.execute_reply.started":"2024-01-22T19:17:48.736936Z","shell.execute_reply":"2024-01-22T19:17:49.428554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spending Features","metadata":{}},{"cell_type":"markdown","source":"Going back to the spending features, we will be capping the shopping columns at 3 standard deviations above the mean. First, we will impute the na values with 0, on the assumption that missing spending data means that no money was spent.\n\n*Note: This would need to be investigated further if assumption was incorrect (we could estimate the spendings based on other variables, e.g. age).*\n","metadata":{}},{"cell_type":"code","source":"train_test[\"RoomService\"] = train_test[\"RoomService\"].fillna(0)\ntrain_test[\"Spa\"] = train_test[\"Spa\"].fillna(0)\ntrain_test[\"VRDeck\"] = train_test[\"VRDeck\"].fillna(0)\ntrain_test[\"FoodCourt\"] = train_test[\"FoodCourt\"].fillna(0)\ntrain_test[\"ShoppingMall\"] = train_test[\"ShoppingMall\"].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:49.431847Z","iopub.execute_input":"2024-01-22T19:17:49.432401Z","iopub.status.idle":"2024-01-22T19:17:49.445531Z","shell.execute_reply.started":"2024-01-22T19:17:49.432357Z","shell.execute_reply":"2024-01-22T19:17:49.444457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shopping_columns = ['RoomService','Spa','VRDeck','FoodCourt','ShoppingMall']\n\navg = train.loc[:,shopping_columns].mean()      \nstd = train.loc[:,shopping_columns].std(0)\n\nclip = list(avg + 3 * std)\n\ntrain_test.loc[:,shopping_columns] = train_test.loc[:,shopping_columns].clip(upper=clip, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:49.447641Z","iopub.execute_input":"2024-01-22T19:17:49.448588Z","iopub.status.idle":"2024-01-22T19:17:49.478684Z","shell.execute_reply.started":"2024-01-22T19:17:49.448527Z","shell.execute_reply":"2024-01-22T19:17:49.476889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at how each shopping column is correlated with the target variable (with the 0 values removed), there are some differences between the distributions of Transported passengers so we will keep them separate but could experiment with combining them. \n- Spa and VRDeck have fairly similar distributions, especially in their lower and maximum spending bins\n- FoodCourt and ShoppingMall are also fairly similar in how their maximum spend bins are distributed, but across the lower spending bins they are less similar\n","metadata":{"execution":{"iopub.status.busy":"2024-01-14T12:49:44.777932Z","iopub.execute_input":"2024-01-14T12:49:44.778386Z","iopub.status.idle":"2024-01-14T12:49:44.786015Z","shell.execute_reply.started":"2024-01-14T12:49:44.778344Z","shell.execute_reply":"2024-01-14T12:49:44.784393Z"}}},{"cell_type":"code","source":"feature_to_target(train_test[0:train_len], shopping_columns, figsize=(20,10), bins=50, remove_0_values=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:49.480851Z","iopub.execute_input":"2024-01-22T19:17:49.481552Z","iopub.status.idle":"2024-01-22T19:17:52.272694Z","shell.execute_reply.started":"2024-01-22T19:17:49.481492Z","shell.execute_reply":"2024-01-22T19:17:52.271561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whether the passenger spent $0 or the maximum capped value may be useful binary features to add, we'll add them and we can test their usefulness later.","metadata":{}},{"cell_type":"code","source":"train_test['RoomService_0'] = train_test['RoomService'] == 0\ntrain_test['RoomService_Max'] = train_test['RoomService'] == clip[0]\n\ntrain_test['Spa_0'] = train_test['Spa'] == 0\ntrain_test['Spa_Max'] = train_test['Spa'] == clip[1]\n\ntrain_test['VRDeck_0'] = train_test['VRDeck'] == 0\ntrain_test['VRDeck_Max'] = train_test['VRDeck'] == clip[2]\n\ntrain_test['FoodCourt_0'] = train_test['FoodCourt'] == 0\ntrain_test['FoodCourt_Max'] = train_test['FoodCourt'] == clip[3]\n\ntrain_test['ShoppingMall_0'] = train_test['ShoppingMall'] == 0\ntrain_test['ShoppingMall_Max'] = train_test['ShoppingMall'] == clip[4]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:52.274233Z","iopub.execute_input":"2024-01-22T19:17:52.274987Z","iopub.status.idle":"2024-01-22T19:17:52.295754Z","shell.execute_reply.started":"2024-01-22T19:17:52.274935Z","shell.execute_reply":"2024-01-22T19:17:52.294172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PassengerID and Cabin","metadata":{}},{"cell_type":"markdown","source":"PassengerId is a combination of the feature Group and GroupID so we will split that. Similarly, Cabin is a combination of Deck, Num and Side so we will also split those into features to see what is useful.","metadata":{}},{"cell_type":"code","source":"# split PassengerId into Group and GroupID\ntrain_test[\"Group\"] = train_test ['PassengerId'].str[0:4].astype('int')\ntrain_test[\"GroupID\"] = train_test ['PassengerId'].str[-2:].astype('int')\n\n# set PassengerID as index\ntrain_test.set_index(\"PassengerId\", inplace = True)\n\n# Split Cabin into Deck/Num/Side ()\ntrain_test[['Deck','Num','Side']] = train_test['Cabin'].str.split('/', expand = True)\n\ntrain_test['Num'] = train_test['Num'].astype(float)\n\n# drop original Cabin features and age for now\ntrain_test = train_test.drop('Cabin', axis = 1)\ntrain_test = train_test.drop('Name', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:52.297438Z","iopub.execute_input":"2024-01-22T19:17:52.297855Z","iopub.status.idle":"2024-01-22T19:17:52.380673Z","shell.execute_reply.started":"2024-01-22T19:17:52.297815Z","shell.execute_reply":"2024-01-22T19:17:52.379189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorica Features","metadata":{}},{"cell_type":"markdown","source":"Looking at the categorical features:\n- The different Homeplanets correlate differently to Transported, with Europa being mostly Transported passengers, Earth being mostly not Transported, and Mars being a fairly even split.\n- CyroSleep looks to be a useful feature with those who were in Cryosleep being very likely to be Transported\n- Destination seems to be not as useful as other features, with there being a fairly even split for TRAPPISE-1e and PSO J318.5-22. 55 Cancri e is the only one where there is a lean more towards Transported.\n- VIP doesn't seem to give us a lot of information\n- Deck and Side are both decent indicators of whether a customer will be Transported so should make useful features\n- GroupID looks to lean more towards Transported as the GroupID increases\n","metadata":{}},{"cell_type":"code","source":"catogorical_columns = ['HomePlanet','CryoSleep','Destination','VIP','Deck','Side', 'GroupID']\n\nfeature_to_target(train_test[0:train_len], catogorical_columns, figsize=(25,20), bins=8)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:52.382273Z","iopub.execute_input":"2024-01-22T19:17:52.3832Z","iopub.status.idle":"2024-01-22T19:17:55.037509Z","shell.execute_reply.started":"2024-01-22T19:17:52.383153Z","shell.execute_reply":"2024-01-22T19:17:55.035893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new features Group and Num both have a large range of values. The Transported split for the Group bands is pretty much 50/50 for all of the bands, so doesn't look like it will be a useful feature. We may be able to use it to help us impute other columns though, so we'll keep it for now. Num does look to be more useful and we can split it into groupings like we did for age, but we need to impute the na values first.","metadata":{}},{"cell_type":"code","source":"group_num_columns = ['Group','Num']\n\nfeature_to_target(train_test[0:train_len], group_num_columns, figsize=(15,6), bins=50)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:55.039125Z","iopub.execute_input":"2024-01-22T19:17:55.039529Z","iopub.status.idle":"2024-01-22T19:17:56.313664Z","shell.execute_reply.started":"2024-01-22T19:17:55.03949Z","shell.execute_reply":"2024-01-22T19:17:56.312623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Group vs Num","metadata":{}},{"cell_type":"markdown","source":"If we plot Group against Num we can see there is a fairly linear trend for each of the Deck and Side combinations. From this we should be able to impute na values for Num based on linear models for each Deck/Side combination. Before we do that let's impute na values for Deck and Side so we can predict all values.","metadata":{}},{"cell_type":"code","source":"train_test['Deck_Side'] = train_test['Deck'] + '_' + train_test['Side']\n\nsns.scatterplot(train_test, x='Group', y='Num', hue='Deck_Side')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:56.315461Z","iopub.execute_input":"2024-01-22T19:17:56.315892Z","iopub.status.idle":"2024-01-22T19:17:58.045494Z","shell.execute_reply.started":"2024-01-22T19:17:56.31585Z","shell.execute_reply":"2024-01-22T19:17:58.043039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputing Values","metadata":{}},{"cell_type":"markdown","source":"It looks like Groups almost always stay on the same Deck, with only a small proportion being across 2 and an even smaller proportion across 3, so we can use group to infer deck when possible. We may be able to do the same mapping exercise in other places so we'll create a function that will do this for us.","metadata":{"execution":{"iopub.status.busy":"2024-01-18T19:58:00.756102Z","iopub.execute_input":"2024-01-18T19:58:00.756825Z","iopub.status.idle":"2024-01-18T19:58:00.762268Z","shell.execute_reply.started":"2024-01-18T19:58:00.756786Z","shell.execute_reply":"2024-01-18T19:58:00.760978Z"}}},{"cell_type":"code","source":"decks_per_group = (pd.crosstab(train_test['Group'], train_test['Deck']) > 0).sum(axis=1).value_counts()\n\ndecks_per_group.plot(kind='bar')\n\nplt.xlabel('Numer of Decks per Group')\nplt.xticks(rotation=0)\nplt.ylabel('Count')\n\nplt.title('Count of the Number of Decks per Group')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:58.048048Z","iopub.execute_input":"2024-01-22T19:17:58.049118Z","iopub.status.idle":"2024-01-22T19:17:58.421689Z","shell.execute_reply.started":"2024-01-22T19:17:58.049049Z","shell.execute_reply":"2024-01-22T19:17:58.420143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def impute_from_mapping(df, impute_column, mapping_column, mapping):\n    \n    column_na = df[impute_column].isna()\n    column_na_before = column_na.sum()\n    \n    df.loc[column_na, impute_column] = df.loc[column_na, mapping_column].map(mapping)\n    \n    column_na_after = df[impute_column].isna().sum()\n    \n    print(f'The Nan values for {impute_column} were reduced from {column_na_before} to {column_na_after}')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:58.423431Z","iopub.execute_input":"2024-01-22T19:17:58.423862Z","iopub.status.idle":"2024-01-22T19:17:58.433844Z","shell.execute_reply.started":"2024-01-22T19:17:58.423819Z","shell.execute_reply":"2024-01-22T19:17:58.431984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deck_group_ct = pd.crosstab(train_test['Group'], train_test['Deck'])\n\ndeck_group_mapping = deck_group_ct.idxmax(axis=1).to_dict()\n\ntrain_test = impute_from_mapping(train_test, 'Deck', 'Group', deck_group_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:58.436349Z","iopub.execute_input":"2024-01-22T19:17:58.43683Z","iopub.status.idle":"2024-01-22T19:17:58.597887Z","shell.execute_reply.started":"2024-01-22T19:17:58.436786Z","shell.execute_reply":"2024-01-22T19:17:58.596412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've still got 162 na values for Deck to deal with. If we look at the combination feature of HomePlanet and Destination, there is some correlation with Deck and we can take the most common combination to fill in the remaining na values, e.g. if the passenger is travelling from Mars to TRAPPIST-1e then we will impute Deck F. First we need to impute HomePlanet and Destination, then we can use this mapping to finish the imputing for Deck.","metadata":{}},{"cell_type":"code","source":"train_test['HomePlanetDestination'] = train_test['HomePlanet'] + ' ' + train_test['Destination']\n\nsns.heatmap(pd.crosstab(train_test['HomePlanetDestination'], train_test['Deck']), annot=True, fmt='g')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:58.599682Z","iopub.execute_input":"2024-01-22T19:17:58.600151Z","iopub.status.idle":"2024-01-22T19:17:59.341625Z","shell.execute_reply.started":"2024-01-22T19:17:58.600105Z","shell.execute_reply":"2024-01-22T19:17:59.340232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some Decks that only travelled from one Homeplanet. Based on this we can begin to map the missing HomePlanet values.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(pd.crosstab(train_test['Deck'], train_test['HomePlanet']), annot=True, fmt='g')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:59.343465Z","iopub.execute_input":"2024-01-22T19:17:59.344263Z","iopub.status.idle":"2024-01-22T19:17:59.758287Z","shell.execute_reply.started":"2024-01-22T19:17:59.344214Z","shell.execute_reply":"2024-01-22T19:17:59.756879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"homeplanet_deck_mapping = {'A': 'Europa'\n                      ,'B': 'Europa'\n                      ,'C': 'Europa'\n                      ,'G': 'Earth'\n                      ,'T': 'Europa'\n                      }\n\ntrain_test = impute_from_mapping(train_test, 'HomePlanet', 'Deck', homeplanet_deck_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:59.760303Z","iopub.execute_input":"2024-01-22T19:17:59.760774Z","iopub.status.idle":"2024-01-22T19:17:59.777614Z","shell.execute_reply.started":"2024-01-22T19:17:59.760726Z","shell.execute_reply":"2024-01-22T19:17:59.776071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fill the remaining values with Earth since that is the most common option and there isn't any other obvious ways we can use other fields to impute HomePlanet.","metadata":{}},{"cell_type":"code","source":"train_test[\"HomePlanet\"] = train_test[\"HomePlanet\"].fillna('Earth')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:59.779679Z","iopub.execute_input":"2024-01-22T19:17:59.780782Z","iopub.status.idle":"2024-01-22T19:17:59.789577Z","shell.execute_reply.started":"2024-01-22T19:17:59.78073Z","shell.execute_reply":"2024-01-22T19:17:59.788374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will impute Destination with TRAPPIST-1e since it the most common value.","metadata":{}},{"cell_type":"code","source":"train_test[\"Destination\"] = train_test[\"Destination\"].fillna('TRAPPIST-1e')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:59.791472Z","iopub.execute_input":"2024-01-22T19:17:59.792195Z","iopub.status.idle":"2024-01-22T19:17:59.804828Z","shell.execute_reply.started":"2024-01-22T19:17:59.792141Z","shell.execute_reply":"2024-01-22T19:17:59.803203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have no NA values for HomePlanet and Destination we can impute Deck based on the combination of the two features as above.","metadata":{}},{"cell_type":"code","source":"train_test['HomePlanetDestination'] = train_test['HomePlanet'] + ' ' + train_test['Destination']\n\ndeck_hpd_ct = pd.crosstab(train_test['HomePlanetDestination'], train_test['Deck'])\n\ndeck_hpd_mapping = deck_hpd_ct.idxmax(axis=1).to_dict()\n\ntrain_test = impute_from_mapping(train_test, 'Deck', 'HomePlanetDestination', deck_hpd_mapping)\n\ntrain_test.drop('HomePlanetDestination', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:59.815924Z","iopub.execute_input":"2024-01-22T19:17:59.81682Z","iopub.status.idle":"2024-01-22T19:17:59.86958Z","shell.execute_reply.started":"2024-01-22T19:17:59.816768Z","shell.execute_reply":"2024-01-22T19:17:59.868207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking back to the Num/Age graph we now need to make sure that Side has no na values so we can make out Num linear model predictions.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(train_test, x='Group', y='Num', hue='Deck_Side')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:17:59.871515Z","iopub.execute_input":"2024-01-22T19:17:59.872009Z","iopub.status.idle":"2024-01-22T19:18:01.515526Z","shell.execute_reply.started":"2024-01-22T19:17:59.871961Z","shell.execute_reply":"2024-01-22T19:18:01.51423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that all groups are on the same side, so we can infer wide from our group examples.","metadata":{"execution":{"iopub.status.busy":"2024-01-18T20:30:28.797508Z","iopub.execute_input":"2024-01-18T20:30:28.797999Z","iopub.status.idle":"2024-01-18T20:30:28.806059Z","shell.execute_reply.started":"2024-01-18T20:30:28.797954Z","shell.execute_reply":"2024-01-18T20:30:28.804419Z"}}},{"cell_type":"code","source":"decks_per_side = (pd.crosstab(train_test['Group'], train_test['Side']) > 0).sum(axis=1).value_counts()\n\ndecks_per_side.plot(kind='bar')\n\nplt.xlabel('Numer of Sides per Group')\nplt.xticks(rotation=0)\nplt.ylabel('Count')\n\nplt.title('Count of the Number of Sides per Group')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:01.51749Z","iopub.execute_input":"2024-01-22T19:18:01.518073Z","iopub.status.idle":"2024-01-22T19:18:01.808482Z","shell.execute_reply.started":"2024-01-22T19:18:01.51801Z","shell.execute_reply":"2024-01-22T19:18:01.806989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"side_group_ct = pd.crosstab(train_test['Group'], train_test['Side'])\n\nside_group_mapping = side_group_ct.idxmax(axis=1).to_dict()\n\ntrain_test = impute_from_mapping(train_test, 'Side', 'Group', side_group_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:01.81064Z","iopub.execute_input":"2024-01-22T19:18:01.811161Z","iopub.status.idle":"2024-01-22T19:18:01.95599Z","shell.execute_reply.started":"2024-01-22T19:18:01.811109Z","shell.execute_reply":"2024-01-22T19:18:01.954321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have imputed as we can, and since S is the slight majority side, we will impute the remaining 162 values with that.","metadata":{}},{"cell_type":"code","source":"train_test[\"Side\"] = train_test[\"Side\"].fillna('S')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:01.957863Z","iopub.execute_input":"2024-01-22T19:18:01.959197Z","iopub.status.idle":"2024-01-22T19:18:01.968967Z","shell.execute_reply.started":"2024-01-22T19:18:01.959142Z","shell.execute_reply":"2024-01-22T19:18:01.967485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that Deck and Side have no na values we will create the linear regression models needed to predict the missing Num values.","metadata":{}},{"cell_type":"code","source":"def create_linear_models_and_plot():\n    \n    sns.scatterplot(train_test, x='Group', y='Num', hue='Deck_Side')\n\n    deck_sides = (train_test['Deck_Side']).unique()\n\n    deck_sides_models = {}\n\n    for deck_side in deck_sides:\n    \n        deck_mask = (train_test['Deck_Side'] == deck_side) & ~(train_test['Num'].isna())\n    \n        X = train_test.loc[deck_mask,'Group'].values.reshape(-1,1)\n        y = train_test.loc[deck_mask,'Num'].values\n    \n    \n        model = LinearRegression()\n    \n        \n        if len(X) > 0:\n        \n            model.fit(X,y)\n    \n            deck_sides_models[deck_side] = model\n        \n            X_example = np.array([[0],[8000]])\n            \n            y_group = model.predict(X_example)\n            \n            sns.lineplot(x=X_example.reshape(-1), y=y_group)\n            \n            #print(f'Build model for deck side: {deck_side})\n        \n        else:\n        \n            deck_sides_models[deck_side] = 'NA'\n            \n    return deck_sides_models\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:01.97051Z","iopub.execute_input":"2024-01-22T19:18:01.970927Z","iopub.status.idle":"2024-01-22T19:18:01.984773Z","shell.execute_reply.started":"2024-01-22T19:18:01.970872Z","shell.execute_reply":"2024-01-22T19:18:01.982864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test['Deck_Side'] = train_test['Deck'] + '_' + train_test['Side']\n\ndeck_sides_models = create_linear_models_and_plot()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:01.986771Z","iopub.execute_input":"2024-01-22T19:18:01.987405Z","iopub.status.idle":"2024-01-22T19:18:05.552773Z","shell.execute_reply.started":"2024-01-22T19:18:01.987356Z","shell.execute_reply":"2024-01-22T19:18:05.55138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_from_linear_models(deck_sides_models):\n    \n    for deck_sides_model in deck_sides_models.items():\n    \n        column_na_before = train_test['Num'].isna().sum()\n    \n        deck_side = deck_sides_model[0]\n        model = deck_sides_model[1]\n    \n        num_na_deck_side_mask = (train_test['Num'].isna()) & (train_test['Deck_Side'] == deck_side)\n    \n        if (model != 'NA') & (num_na_deck_side_mask.sum() > 0): \n        \n            train_test.loc[num_na_deck_side_mask, 'Num'] = model.predict(train_test.loc[num_na_deck_side_mask, 'Group'].values.reshape(-1,1)).astype(int)\n        \n        \n        column_na_after = train_test['Num'].isna().sum()\n    \n        print(f'The Nan values for Num were reduced from {column_na_before} to {column_na_after}')\n        \n        # set any negative predictions to group 0\n        negative_num_mask = train_test['Num'] < 0\n        \n        train_test.loc[negative_num_mask, 'Num'] = 0","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:05.55468Z","iopub.execute_input":"2024-01-22T19:18:05.555244Z","iopub.status.idle":"2024-01-22T19:18:05.568736Z","shell.execute_reply.started":"2024-01-22T19:18:05.555093Z","shell.execute_reply":"2024-01-22T19:18:05.566831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_from_linear_models(deck_sides_models)\n\ntrain_test.drop('Deck_Side', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:05.570181Z","iopub.execute_input":"2024-01-22T19:18:05.571333Z","iopub.status.idle":"2024-01-22T19:18:05.682052Z","shell.execute_reply.started":"2024-01-22T19:18:05.57128Z","shell.execute_reply":"2024-01-22T19:18:05.680372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binning Num","metadata":{}},{"cell_type":"markdown","source":"We will now translate the Num nuemrical feature into distrete bands, as we did for Age.","metadata":{}},{"cell_type":"code","source":"feature_to_target(train_test[0:train_len], ['Num'], figsize=(15,6), bins=50)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:05.684169Z","iopub.execute_input":"2024-01-22T19:18:05.68462Z","iopub.status.idle":"2024-01-22T19:18:06.370117Z","shell.execute_reply.started":"2024-01-22T19:18:05.684578Z","shell.execute_reply":"2024-01-22T19:18:06.368585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins = np.arange(-1,2001,250)\nnames = ['0-249', '250-499', '500-749', '750-999', '1000-1249', '1250-1499', '1500-1749', '1750-1999']\n\ntrain_test['NumBand'] = pd.cut(train_test['Num'], bins, labels=names)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:06.371891Z","iopub.execute_input":"2024-01-22T19:18:06.372383Z","iopub.status.idle":"2024-01-22T19:18:06.386166Z","shell.execute_reply.started":"2024-01-22T19:18:06.372335Z","shell.execute_reply":"2024-01-22T19:18:06.384403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The different bandings for Num show different patterns in the distribution of Transported, so will be a useful feature. We can the drop the Num column, and also the Group column which we only kept to help impute Num.","metadata":{}},{"cell_type":"code","source":"sns.catplot(data=train_test[0:train_len], x=\"NumBand\", kind=\"count\", hue='Transported')\n\nplt.xticks(rotation=90)\nplt.title('Distribution of Transported Passengers for the new Num Band Feauture')\n\nplt.show()\n\ntrain_test.drop('Num', axis=1, inplace=True)\ntrain_test.drop('Group', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:06.388015Z","iopub.execute_input":"2024-01-22T19:18:06.389888Z","iopub.status.idle":"2024-01-22T19:18:07.078453Z","shell.execute_reply.started":"2024-01-22T19:18:06.389829Z","shell.execute_reply":"2024-01-22T19:18:07.076675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputing Remaining Features","metadata":{}},{"cell_type":"markdown","source":"The remaining columns with na values are CryoSleep and VIP which we will impute with the mode.","metadata":{}},{"cell_type":"code","source":"na_count = train_test.isna().sum()\nna_count[na_count > 0]\nprint(na_count[na_count > 0])","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:07.080854Z","iopub.execute_input":"2024-01-22T19:18:07.081542Z","iopub.status.idle":"2024-01-22T19:18:07.108674Z","shell.execute_reply.started":"2024-01-22T19:18:07.081471Z","shell.execute_reply":"2024-01-22T19:18:07.107033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test[\"CryoSleep\"] = train_test[\"CryoSleep\"].fillna(False)\ntrain_test[\"VIP\"] = train_test[\"VIP\"].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:07.112214Z","iopub.execute_input":"2024-01-22T19:18:07.113538Z","iopub.status.idle":"2024-01-22T19:18:07.130658Z","shell.execute_reply.started":"2024-01-22T19:18:07.113476Z","shell.execute_reply":"2024-01-22T19:18:07.129003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get dummies and split into original train & test set","metadata":{}},{"cell_type":"markdown","source":"We've imputed all of the missing values so we need to prepare the dataset so that it can be modelled by our Random Forest Model. For this we can't have categorical features, so we will translate them into dummy Boolean features.","metadata":{}},{"cell_type":"code","source":"train_test = pd.get_dummies(train_test)\n\nX_train = train_test[0:train_len]\nX_submission = train_test[train_len:]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:07.133072Z","iopub.execute_input":"2024-01-22T19:18:07.134158Z","iopub.status.idle":"2024-01-22T19:18:07.173921Z","shell.execute_reply.started":"2024-01-22T19:18:07.134103Z","shell.execute_reply":"2024-01-22T19:18:07.172345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = X_train['Transported']\nX_train = X_train.drop(['Transported'], axis=1)\n\nX_submission = X_submission.drop(['Transported'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:07.175747Z","iopub.execute_input":"2024-01-22T19:18:07.17619Z","iopub.status.idle":"2024-01-22T19:18:07.18997Z","shell.execute_reply.started":"2024-01-22T19:18:07.176146Z","shell.execute_reply":"2024-01-22T19:18:07.188302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Optuna to find optimal Hyperparamters for a Random Forest Model","metadata":{}},{"cell_type":"markdown","source":"Using Optuna we will define the objective function that we are trying to minimise, which is the log loss of our Random Forest model. As part of this we will define the Hyperparameter space that Optuna will be searching through. We have chosen commonly tuned Hyperparameters to trial to see which ones help our model's performance.\n\nFirst we will train a Random Forest Classifier with the default parameters to give us a benchmark. From that we can see what improvements can be made by using Optuna.\n","metadata":{}},{"cell_type":"code","source":"def train_test_model(model, X, y, test_size=0.3, random_state=1):\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    model.fit(X_train, y_train)\n        \n    y_pred = model.predict(X_test)\n        \n    logloss = log_loss(y_test, y_pred)\n    \n    return logloss","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:07.192186Z","iopub.execute_input":"2024-01-22T19:18:07.193567Z","iopub.status.idle":"2024-01-22T19:18:07.204714Z","shell.execute_reply.started":"2024-01-22T19:18:07.193491Z","shell.execute_reply":"2024-01-22T19:18:07.203445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = 3548\n\ndefault_model = RandomForestClassifier(random_state=random_state)\n\ndefault_logloss = train_test_model(default_model, X_train, y_train, 0.3, random_state)\n\nprint(f'Using the default Random Forest Parameters we obtain a log loss score of {default_logloss:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:07.206593Z","iopub.execute_input":"2024-01-22T19:18:07.207134Z","iopub.status.idle":"2024-01-22T19:18:08.327224Z","shell.execute_reply.started":"2024-01-22T19:18:07.207081Z","shell.execute_reply":"2024-01-22T19:18:08.325602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be performing 3 fold cross validation and computing the mean log loss in order to avoid overfitting our hyperparameters, which can lead to poorer performance on unseen data. The hyperparameters we will be tuning are:\n- n_estimators - the number of Decision Trees used in the Random Forest Ensemble\n- max_depth - the Maximum depth of the individual Decision Trees\n- min_samples_leaf - the minimum number of samples in a leaf node created by a split needed for a split to occur\n- min_samples_split - the minimum number of samples in a leaf node needed to split it further\n- max_features - The maximum number of features used at each split when finding the optimal splitting feature\n\nOptuna uses a Bayesian optimization algorithm to learn about how the available hyperparameters affect to objective function, in this case the log loss score average across 3 fold cross validation. With each successive trial it learns more information and uses this information to suggest new values for each hyperparameter that it thinks will lead to further improvement.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \n    folds = 3\n    ## define hyperparatmers\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 1, 1000)\n            ,'max_depth': trial.suggest_int('max_depth', 1, 250)\n            ,'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 25)\n            ,'min_samples_split': trial.suggest_int('min_samples_split', 2, 50)\n            ,'max_features': trial.suggest_categorical('max_features', choices = ['sqrt', 'log2', None])\n    }\n    \n    # train model\n    model = RandomForestClassifier(**params, random_state = np.random.seed(50))\n    \n    \n    kf = KFold(n_splits=folds)\n    \n    logloss = np.empty(folds)\n    \n    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n        \n        model.fit(X_train.iloc[train_index,:], y_train.iloc[train_index])\n        \n        y_pred = model.predict(X_train.iloc[test_index,:])\n        \n        logloss[i] = log_loss(y_train.iloc[test_index], y_pred)\n        \n    logloss_mean = np.mean(logloss)\n    \n    return logloss_mean","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:18:08.328885Z","iopub.execute_input":"2024-01-22T19:18:08.329444Z","iopub.status.idle":"2024-01-22T19:18:08.344337Z","shell.execute_reply.started":"2024-01-22T19:18:08.329391Z","shell.execute_reply":"2024-01-22T19:18:08.342465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's run 100 trials and see how hyperparameter tuning can help increase the accuracy of our model. We will set the seed for reproducibility.","metadata":{}},{"cell_type":"code","source":"sampler = TPESampler(seed=5684) \n\nstudy = optuna.create_study(direction='minimize',sampler=sampler)\n\nstudy.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:39:05.431724Z","iopub.execute_input":"2024-01-22T19:39:05.432405Z","iopub.status.idle":"2024-01-22T20:08:28.474348Z","shell.execute_reply.started":"2024-01-22T19:39:05.432341Z","shell.execute_reply":"2024-01-22T20:08:28.472878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can set that as the number of trials increases, Optuna makes steady decreases in the log loss while experimenting with different combinations of hyperparameters. Initially, we see larger decreases in the log loss, but as the number of trials increases, the improvements decrease in size as we converge on one of optimal combinations of hyperparameters.\n\n*Note: Optuna searches for the most optimal combination of hyperparameters in the supplied hyperparameter space, but similar to how ML models can, Optuna can find local minima rather than the global minima so there may be a better combination of hyperparameters available. This method is just much quicker at converging on efficient combinations than more traditional methods like Grid Search.*\n","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:16:06.898086Z","iopub.execute_input":"2024-01-22T20:16:06.898668Z","iopub.status.idle":"2024-01-22T20:16:06.945496Z","shell.execute_reply.started":"2024-01-22T20:16:06.898622Z","shell.execute_reply":"2024-01-22T20:16:06.94418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With Optuna's plot_slice plot we can see how each hyperparameter performed individually. \n\nFor this study Max Depth converged around the middle of the defined range (at 119), although in the range ~90 to ~190 it performed similarly. Although you may expect deeper trees to perform better, this can 1. lead to overfitting in which case we need a large number of estimators to compensate, and 2. may not even be possible based on the sample size and splitting restrictions set by Min Sample Leaf and Min Sample Split, which may be why it settled at this middle value in our range.\n\nUsing the Square Root method was by far the best way to decide on the number of features to look at for each split. Log2 was second with no method (i.e. using all features) performing the worse, which is expected since using all features can result in the model only using the most predominant features and not using the less predominant ones, which can still give the model useful information.\n\nMin Sample Leaf and Min Sample Split both tended towards the lower values, where we allow the trees to keep splitting until we have very few samples left in outer nodes, with Min Sample Leaf showing a strong correlation between it's value and the log loss. This on it's own can cause overfitting of the model, since it begins to learn patterns in very small subsets of the data which can tend not to apply to unseen data, but having a large number of estimators can counteract this since each models overfitting can be \"cancelled out\" by using a large number of models.\n\nWe see that the model found a number of estimators of 767 in the trials we did, although looking at the distribution, the model performed similarly well for a large range of values. In the case of wanting the most optimal model accuracy, then taking this peak value would make sense, but in cases where you are looking to deploy models to production, we may be better off using a lower number of estimators. This is because although we may see a slight decrease in performance, the model training time could be decreased by using fewer estimators, and so may be a better production solution.\n\n*Note: This view doesn't let you view the interactions between hyperparameters since it only focuses on one hyperparameter at a time, so it's not always clear from the graphs which trials were of result of the hyperparameter being plotted, or the other hyperparameters being chosen which you can't see, for example although we see a strong correlation between Min Samples Leaf and the log loss, we see values of MSL the same as the optimal value found with much worse performance. In this case, it is likely on these trials, the other hyperparameters were set to poor performing values. For looking at the overall trend though, it is useful to get a view of the general trend for each of the hyperparameters.*\n\n*Additionally, as mentioned previously, we may see differing results if we re-ran the trial since the optimal set of hyperparameters found could sit in a local optima, so there may be different sets of hyperparameters that vary from ours but produce a similar peak performance.*","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:16:12.976178Z","iopub.execute_input":"2024-01-22T20:16:12.976714Z","iopub.status.idle":"2024-01-22T20:16:13.150577Z","shell.execute_reply.started":"2024-01-22T20:16:12.976665Z","shell.execute_reply":"2024-01-22T20:16:13.149187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can view the interactions between two hyperparameters more specifically using the plot_contour plot.Below we can see firstly the interactions between Max Depth and Number of Estimators, and how these two hyperparameters affect the model's log loss score. \n\nWe can see that perfomance for low values of both hyperparameters is worse (has a lighter gradient), which is as expected since the model could have less availabiltiy to model all the complexity within the dataset in these cases. For high values of Max Depth, the performance seemed to be largely independant on the Number of Estimators, since we see a light gradient across this side (apart from 2 individual trials which appear to be outliers and whose performance may have been impacted by other hyperparameters).\n\nThe optimal combinations sit where Max Depth and Number of Estimators are around the centre of their ranges, but we can see there is a range quite an area of dark blue contours around there where we see good model performance.","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_contour(study, params=[\"max_depth\", \"n_estimators\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:36:37.664743Z","iopub.execute_input":"2024-01-22T20:36:37.665854Z","iopub.status.idle":"2024-01-22T20:36:37.806818Z","shell.execute_reply.started":"2024-01-22T20:36:37.665797Z","shell.execute_reply":"2024-01-22T20:36:37.80549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Min Sample Leaf and Min Sample Split we see, similarly to what we observed in their slice plots, that a combination of low values for both provided the best performance. And trials having high values for both saw poor performance (white contours), whereas a high value of one and a low value of the other saw slight improvement (light blue contours). \n\nCompared to the above contour plot, it's clear that these two had a much more direct impact on the log loss, so we'd want to make sure we stuck with the lower values in all cases.","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_contour(study, params=[\"min_samples_leaf\", \"min_samples_split\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:16:52.885609Z","iopub.execute_input":"2024-01-22T20:16:52.886195Z","iopub.status.idle":"2024-01-22T20:16:52.932845Z","shell.execute_reply.started":"2024-01-22T20:16:52.886137Z","shell.execute_reply":"2024-01-22T20:16:52.931327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optuna also allows you to see which hyperparameters were most important in improving the objective function.\n\nIn our case Min Samples Leaf as by far the most important which is expected since it's slice plot showed the most correlated distribution against the log loss, whereas other plots had a more spread out distribution. From this we can infer that the dataset had a reasonable amount of complexity between features and target, because we required leaf nodes with a small number of individual passengers in order to model the patterns in the dataset.\n\nMax Features was also deemed an important feature since there were many examples of the square root method performing better than the two others.\n\nMax Depth and Min Samples Split were somewhat important, whereas Number of Estimators was deemed unimportant, i.e. Optuna deemed that varying this hyperparameter had little effect on the log loss, and most of the improvement was driven by the other hyperparameters, which can be seen by the high performance across a range of Max Depth values.\n","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:54:36.199935Z","iopub.execute_input":"2024-01-22T20:54:36.200463Z","iopub.status.idle":"2024-01-22T20:54:39.940457Z","shell.execute_reply.started":"2024-01-22T20:54:36.200413Z","shell.execute_reply":"2024-01-22T20:54:39.9391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have optimised our hyperparameters, we can take the parameters from the most optimal trial and use those to train a model and compare it's results to our default model's results.","metadata":{}},{"cell_type":"code","source":"best_trial = study.best_trial\nparams = best_trial.params","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:54:34.15634Z","iopub.execute_input":"2024-01-22T20:54:34.156871Z","iopub.status.idle":"2024-01-22T20:54:34.163746Z","shell.execute_reply.started":"2024-01-22T20:54:34.156828Z","shell.execute_reply":"2024-01-22T20:54:34.162211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_model = RandomForestClassifier(**params)\n\noptimal_logloss = train_test_model(optimal_model, X_train, y_train, 0.3, random_state)\n\nprint(f'Using the optimal hyperparemters found by Optuna we obtain a log loss score of {optimal_logloss:.2f}.\\n')\n\nimprovement = 100 * (default_logloss - optimal_logloss)/default_logloss\n\nprint(f'This optimal log loss score of {optimal_logloss:.2f} corresponds to a {improvement:.1f}% improvement compared to the default model\\'s log loss of {default_logloss:.2f}.')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T20:54:44.221814Z","iopub.execute_input":"2024-01-22T20:54:44.222355Z","iopub.status.idle":"2024-01-22T20:54:50.842955Z","shell.execute_reply.started":"2024-01-22T20:54:44.222306Z","shell.execute_reply":"2024-01-22T20:54:50.841427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the kind of improvement we might expect with Optuna. We would always expect the highest increases in performance will come from proper preprocessing and feature engineering, but we can then often make further improvements to our models using hyperparameter tuning, which Optuna makes much easier to execute and visualise than traditional methods.\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T11:22:13.092137Z","iopub.execute_input":"2024-01-20T11:22:13.092512Z","iopub.status.idle":"2024-01-20T11:22:13.099523Z","shell.execute_reply.started":"2024-01-20T11:22:13.092479Z","shell.execute_reply":"2024-01-20T11:22:13.097943Z"}}},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"X_submission['Transported'] = optimal_model.predict(X_submission)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save to csv\nX_submission['Transported'].to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-01-14T11:36:45.51246Z","iopub.status.idle":"2024-01-14T11:36:45.512901Z","shell.execute_reply.started":"2024-01-14T11:36:45.512658Z","shell.execute_reply":"2024-01-14T11:36:45.512702Z"},"trusted":true},"execution_count":null,"outputs":[]}]}